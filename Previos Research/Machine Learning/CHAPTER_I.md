# CHAPTER I
## THE PROBLEM AND ITS BACKGROUND

### Proposed Title
**Machine Learning-Based Early Warning System for Predicting Late or Incomplete Faculty Portfolio Submissions in a Web-Based Faculty Portfolio Management System**

---

## 1.1 Background of the Study

Higher education institutions require faculty members to submit instructional and academic portfolio documents for monitoring, evaluation, and compliance. These documents commonly include course syllabi, instructional materials, and other required evidence of teaching activities. In many institutions, submission monitoring is still handled manually through spreadsheets, message follow-ups, and periodic checks by department chairs. This process is time-consuming and often reactive, where interventions happen only after delays or incomplete submissions are already observed.

With the increasing volume of academic data generated by web-based systems, machine learning (ML) offers opportunities to shift monitoring from reactive to proactive. Instead of waiting for missed deadlines, an ML model can estimate the likelihood that a faculty member or class offering may become late or incomplete based on historical and behavioral patterns. This allows chairs and administrators to provide targeted support earlier.

The Faculty Portfolio Management System developed by the researcher already captures submission-related records such as class offerings, submission status, completion progress, and review history. These data elements provide a practical foundation for predictive modeling. Integrating an ML-based early warning component into the existing system can improve decision-making, reduce manual monitoring burden, and support timely submission compliance.

This study is therefore proposed to design, develop, and evaluate a machine learning-based early warning model that predicts at-risk portfolio submissions and integrates its outputs into the current web application for operational use.

---

## 1.2 Statement of the Problem

### General Problem
The current monitoring of faculty portfolio submissions is mostly manual and reactive, resulting in delayed identification of at-risk submissions and limited opportunity for timely intervention.

### Specific Problems
This study seeks to answer the following questions:

1. What historical and behavioral factors are significantly associated with late or incomplete faculty portfolio submissions?
2. Can machine learning models accurately predict at-risk submissions before the deadline?
3. Which supervised machine learning algorithm provides the best predictive performance in terms of accuracy, precision, recall, F1-score, ROC-AUC, and PR-AUC?
4. How can prediction results be integrated into the existing Faculty Portfolio Management System to support chair-level monitoring and intervention?
5. Is there a statistically significant difference in predictive performance among the selected machine learning algorithms?

---

## 1.3 Objectives of the Study

### General Objective
To develop and evaluate a machine learning-based early warning system for predicting late or incomplete faculty portfolio submissions and integrate it into a web-based Faculty Portfolio Management System.

### Specific Objectives
1. To identify relevant predictive features from faculty portfolio and class offering records.
2. To prepare and label a historical dataset for supervised machine learning.
3. To train and compare selected classification algorithms (e.g., Logistic Regression, Random Forest, XGBoost/Gradient Boosting).
4. To evaluate model performance using standard classification metrics (accuracy, precision, recall, F1-score, ROC-AUC, and PR-AUC), including calibration reliability.
5. To deploy the best-performing model as a risk scoring feature within the existing system.
6. To determine whether differences in predictive performance among selected models are statistically significant.

---

## 1.4 Hypothesis of the Study (Optional for Quantitative/Experimental Format)

**Null Hypothesis (H0):**
There is no significant difference in predictive performance among the selected machine learning algorithms in identifying at-risk faculty portfolio submissions.

**Alternative Hypothesis (H1):**
There is a significant difference in predictive performance among the selected machine learning algorithms in identifying at-risk faculty portfolio submissions.

---

## 1.5 Significance of the Study

This study may benefit the following stakeholders:

1. **Department Chairs**: Provides an early warning dashboard to prioritize follow-ups and interventions for at-risk submissions.
2. **Faculty Members**: Encourages timely completion through earlier guidance and reduced last-minute compliance pressure.
3. **Academic Administrators**: Improves monitoring efficiency, reporting quality, and policy implementation for document compliance.
4. **Institution**: Supports data-driven governance and quality assurance initiatives.
5. **Future Researchers**: Serves as a reference for applying machine learning in academic workflow and compliance systems.
6. **System Developers**: Demonstrates practical integration of predictive analytics into a Laravel-based academic management platform.

---

## 1.6 Scope and Delimitation of the Study

### Scope
1. The study focuses on faculty portfolio submissions recorded in the existing web-based system.
2. It covers data-driven prediction of submission risk status (e.g., likely complete/on-time vs likely late/incomplete).
3. It includes model training, evaluation, and integration of a risk indicator module in the current application.
4. It uses historical records from selected academic years available in the database.

### Delimitations
1. The model output is intended for decision support and does not replace final human judgment.
2. Prediction performance depends on available and quality historical data.
3. The study is limited to the institution/system context where the data is collected.
4. Real-time external factors (e.g., personal emergencies, policy changes) may not be fully represented in the model features.

---

## 1.7 Conceptual Framework (Input-Process-Output)

### Input
- Historical portfolio submission data
- Class offering details (term, section, academic year, subject load)
- Faculty-related activity indicators within the system
- Submission status labels (on-time/complete vs late/incomplete)

### Process
1. Data extraction and preprocessing
2. Feature engineering and labeling
3. Model training and validation
4. Performance evaluation and model selection
5. System integration of risk scoring and warning indicators

### Output
- A validated ML classification model
- An integrated early warning module in the Faculty Portfolio Management System
- Risk-based monitoring support for chairs and administrators

---

## 1.8 Definition of Terms

1. **Faculty Portfolio**: A collection of required instructional and academic documents submitted by faculty members for a specific class offering.
2. **Early Warning System**: A feature that identifies potential submission risks before deadline violations occur.
3. **At-Risk Submission**: A predicted case with high probability of being late, incomplete, or non-compliant.
4. **Machine Learning (ML)**: A computational approach that learns patterns from historical data to generate predictions.
5. **Classification Model**: An ML model that assigns records into predefined classes (e.g., at-risk vs not at-risk).
6. **Risk Score**: A probability-based value representing the likelihood of delayed or incomplete submission.
7. **Predictive Analytics**: The use of statistical and ML techniques to forecast outcomes from historical data.
8. **Model Evaluation Metrics**: Measures such as accuracy, precision, recall, F1-score, and ROC-AUC used to assess prediction performance.
9. **PR-AUC (Area Under the Precision-Recall Curve)**: A performance metric used in imbalanced classification problems to evaluate how well the model identifies positive (at-risk) cases.
10. **Calibration**: The degree to which predicted risk probabilities match observed outcome frequencies.
