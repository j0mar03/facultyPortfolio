**Study ID/Title:** A. Kumar Veerasamy et al. (2020) - "Using early assessment performance as early warning signs to identify at-risk students in programming courses"

**Objective/Target variable:**

- To identify at-risk students early in the semester (specifically by Week 3).
- **Target variable:** Final Exam Grade (FEG) binary classification:
    - **At-risk (Class 0):** Failed to sit exam or scored < 60 marks (Grades 0 and 1).
    - **Not-at-risk (Class 1):** Scored ≥ 60 marks (Grades 2 to 5).

**Dataset:**

- **Source:** ViLLE Learning Management System (LMS) database from an "Introduction to Programming" course at the University of Turku, Finland.
- **Sample size (n):**
    - **Total:** 217 students across three years used for modeling.
    - **Training (2016):** n = 66.
    - **Validation (2017):** n = 68.
    - **Testing (2018):** n = 83 (excluding students identified by the manual threshold rule).
- **Features (count + key features):** 2 features used for the predictive model:
    - **Homework (HE):** Scores from the first two weeks.
    - **Demo Exercises (DE):** Scores from the first two weeks.
- **Class balance (counts or %):**
    - **2016 (Training):** At-risk: 28 (42%); Not-at-risk: 38 (58%).
    - **2017 (Validation):** At-risk: 16 (24%); Not-at-risk: 52 (76%).
    - **2018 (Testing):** At-risk: 43 (52%); Not-at-risk: 40 (48%).

**Preprocessing:**

- **Missing values handling:** NR (Data includes only students who completed formative assessments and the final exam for the model; those scoring ≤25% in formative assessments were handled by a separate manual rule).
- **Encoding/scaling:** NR
- **Feature selection (if any):** Features selected based on prior study and ease of access for instructors.
- **Imbalance handling (SMOTE/class weights/undersampling/etc.):** NR (The authors note the imbalanced nature of the 2017 validation set affected performance, but no handling technique is reported).

**Models compared:**

- **Method 1:** Classification tree analysis (Manually created rule: If formative assessment score ≤ 25% in first two weeks -> At-risk).
- **Method 2:** Random Forest Classification (Parsimonious model).
- _Note: The authors mention exploring Naïve Bayes, C5.0, and SVM in preliminary work, but only report detailed results for Random Forest._

**Validation design:**

- **Train/test split OR k-fold (k=?):**
    - **Training:** 10-fold cross-validation on the 2016 dataset.
    - **Validation:** 2017 dataset used to verify performance.
    - **Testing:** 2018 dataset used as "unknown data".
- **Any external validation?:** Tested on subsequent cohorts (2017 and 2018).

**Metrics reported:**

- **Accuracy:** Overall prediction accuracy reported.
    - Training (2016): 72.73%
    - Validation (2017): 52.94%
    - Testing (2018): 59.64%.
- **Precision:** NR
- **Recall:** Reported as "At-risk prediction accuracy" (Sensitivity).
    - Training (2016): 86.84%
    - Validation (2017): 37.50%
    - Testing (2018): 76.74%.
- **F1-score:** NR
- **ROC-AUC:**
    - Training (2016): 0.73
    - Validation (2017): 0.48
    - Testing (2018): 0.58.
- **PR-AUC:** NR
- **Calibration (Brier/calibration curve):** NR
- **Best model + best values:** Random Forest on Training set (Accuracy: 72.73%, Recall: 86.84%, AUC: 0.73).

**Statistical test/comparison:** NR

**Explainability:**

- **Feature importance/SHAP/LIME/etc.:** NR (The model is described as parsimonious using only two variables).
- **Top predictors:** Homework (HE) and Demo Exercises (DE) scores in the first two weeks.

**Key findings (2-4 bullets):**

- Students scoring ≤25% in formative assessments during the first two weeks are highly likely (approx. 90%) to fail or not attend the final exam.
- Using only the first two weeks of homework and demo exercise scores, the Random Forest model identified 77% of at-risk students in the test dataset (2018).
- Combining the manual threshold rule (Method 1) and the Random Forest model (Method 2) allowed for the identification of approximately 83% of all at-risk students early in the semester.

**Limitations (from paper):**

- Sample size was not sufficiently large to generalize findings.
- The model used only the first two weeks of data; however, student learning is dynamic and performance may change later in the semester.
- The validation dataset (2017) was skewed (imbalanced classes), leading to poor performance on that specific year.
- Other causal factors like prior programming knowledge, motivation, and self-esteem were not included.

**Deployment/implementation notes (if any):**

- The study suggests visualizing these early warning signs (categorizing students as "at-risk", "marginal", "good") for instructors to provide timely interventions and for students to self-regulate.