# CHAPTER III
## RESEARCH METHODOLOGY

This chapter presents the research design, sources of data, instrumentation, data preparation and modeling procedures, statistical treatment, ethical considerations, and analysis framework used in developing and evaluating a Machine Learning-Based Early Warning System for predicting late or incomplete faculty portfolio submissions in a web-based Faculty Portfolio Management System.

## 3.1 Research Design

This study uses a **quantitative predictive research design** with a **developmental system integration component**. The design is appropriate because the main objective is to build, test, and compare machine learning classification models using historical institutional data to predict at-risk submissions before deadlines.

The study is quantitative because it relies on numerical data extracted from system records, objective performance metrics, and statistical model comparison. It is predictive because it estimates the probability of future outcomes (late or incomplete submission risk) from historical patterns.

The methodological flow follows a structured machine learning analytics process:

1. Data extraction from the Faculty Portfolio Management System database  
2. Data cleaning, preprocessing, and feature engineering  
3. Supervised model training and validation  
4. Model evaluation and statistical comparison  
5. Integration of the best model into the web application as an early warning module  

This design supports evidence-based decision-making and is aligned with system-oriented IT research where algorithm performance and operational utility are both required outputs.

## 3.2 Sources of Data

The study uses institutional data generated by the Faculty Portfolio Management System. Data sources are limited to records relevant to portfolio submission behavior and prediction target labeling.

### 3.2.1 Primary Data Sources

Primary data are extracted from system tables and logs, including:

1. Faculty and class offering records (academic year, term, section, subject assignment)
2. Portfolio records (status, submission timestamp, completion progress)
3. Portfolio item records (required document presence/completeness indicators)
4. Workflow and review status history (submitted, approved, changes requested, rejected)
5. Time-related activity indicators captured by the system (where available)

### 3.2.2 Secondary Data Sources

Secondary sources include:

1. Institutional policy documents on portfolio submission requirements and deadlines
2. Literature on educational analytics, risk prediction, and early warning systems
3. Prior studies on supervised classification models and model evaluation metrics

## 3.3 Population, Unit of Analysis, and Sampling

### 3.3.1 Population

The population consists of historical faculty portfolio submission instances recorded in the system across selected academic years.

### 3.3.2 Unit of Analysis

The unit of analysis is a **faculty-class offering portfolio instance** for a specific academic term.

### 3.3.3 Sampling Technique

The study uses **complete enumeration of eligible historical records** within the defined time window, subject to inclusion and exclusion criteria.

### 3.3.4 Inclusion Criteria

1. Records with complete identity linkage to class offering and portfolio
2. Records within the selected academic years included in the study scope
3. Records with sufficient fields required for feature construction and target labeling

### 3.3.5 Exclusion Criteria

1. Corrupted or duplicate records
2. Records with irrecoverable missing critical fields
3. Test or non-production records

## 3.4 Research Instrumentation

This study does not use a perception survey instrument. The principal instrument is a **researcher-defined data extraction and modeling specification** composed of:

1. Target variable definition (binary class: at-risk vs not at-risk)
2. Feature set specification (submission behavior, completion and timing indicators, contextual variables)
3. Preprocessing rules (missing value handling, encoding, scaling where needed)
4. Modeling configuration (algorithms, hyperparameter search space, validation strategy)
5. Evaluation protocol (metrics and statistical comparison procedures)

Automated scripts within the development environment serve as procedural instruments for reproducible preprocessing, training, evaluation, and reporting.

## 3.5 Target Label Definition

The dependent variable is **submission risk status**.

For this study, a record is labeled **At-Risk (1)** if any of the following is observed:

1. Submission is late relative to policy deadline; or
2. Submission is incomplete based on required portfolio document criteria by the evaluation cutoff.

Otherwise, the record is labeled **Not At-Risk (0)**.

Labeling rules will be explicitly documented to ensure reproducibility and consistency.

## 3.6 Feature Engineering

Independent variables may include, but are not limited to:

1. Academic context: term, academic year, subject type, section load characteristics
2. Portfolio progress indicators: completed required items, missing required items, completion ratio
3. Historical behavior indicators: prior completion patterns, prior delays, prior review outcomes
4. Temporal indicators: time elapsed before deadline, submission timing behavior
5. Workflow indicators: counts of revisions or changes requested (if available before cutoff)

Only features available prior to the prediction point are included to prevent data leakage.

## 3.7 Data Preparation Procedure

Data preparation follows a reproducible pipeline:

1. Extract eligible records from production-safe data export
2. Remove duplicates and invalid records
3. Handle missing values using predefined imputation rules
4. Encode categorical variables (one-hot or ordinal encoding as appropriate)
5. Scale numerical variables where algorithmically required
6. Construct training-ready dataset with feature matrix and binary target
7. Perform class distribution analysis and imbalance handling

To preserve temporal realism, data partitioning uses **time-aware splitting** where older academic periods are used for training and newer periods are reserved for validation/testing.

## 3.8 Model Development Procedure

The following supervised classification algorithms are developed and compared:

1. Logistic Regression (baseline interpretable model)
2. Random Forest Classifier
3. Gradient Boosting / XGBoost-compatible classifier (subject to environment availability)

Model training includes:

1. Hyperparameter tuning (grid search or randomized search)
2. Cross-validation on training data (time-aware folds where applicable)
3. Class imbalance handling through class weights and/or resampling

The final model is selected based on metric performance and operational interpretability for deployment in a chair-facing early warning dashboard.

## 3.9 Statistical Treatment of Data

The statistical treatment is fully quantitative and divided into descriptive, predictive, and comparative evaluation.

### 3.9.1 Descriptive Statistics

1. Frequency and percentage distributions for class labels and key categorical features  
2. Measures of central tendency and dispersion for numerical predictors  
3. Missing value profiling and class imbalance ratio reporting  

Formula for percentage:

\[
P = \frac{n}{N} \times 100
\]

Where:  
`P` = percentage  
`n` = frequency of cases in a category  
`N` = total number of cases  

### 3.9.2 Predictive Performance Metrics

For each model, the following are computed on validation and test sets:

1. Accuracy  
2. Precision  
3. Recall (Sensitivity)  
4. F1-score  
5. ROC-AUC  
6. PR-AUC (especially for imbalanced classes)  
7. Confusion Matrix (TP, FP, TN, FN)  

Primary selection emphasis is placed on **Recall, F1-score, and ROC-AUC/PR-AUC** because early warning tasks prioritize identifying true at-risk cases while controlling false alarms.

### 3.9.3 Calibration and Risk Reliability

To assess probability reliability of risk scores:

1. Brier Score  
2. Calibration plot / expected calibration behavior  

This ensures predicted probabilities are usable for intervention thresholds in actual operations.

### 3.9.4 Statistical Comparison of Competing Models

To determine whether model performance differences are statistically meaningful:

1. **McNemarâ€™s Test** for paired classification error comparison on the same test set
2. Optional AUC significance comparison (e.g., DeLong-type comparison) when applicable

Significance level is set at:

\[
\alpha = 0.05
\]

## 3.10 Data Analysis Procedure

Data analysis is conducted in sequence:

1. Build and validate labeled dataset
2. Generate baseline model and benchmark metrics
3. Train and tune candidate models
4. Evaluate all models on hold-out test data
5. Compare model performance statistically
6. Select final model based on predictive strength and deployment practicality
7. Produce feature importance and model interpretation outputs for chair/admin use

Outputs include tables and figures for:

1. Dataset profile
2. Metric comparison across algorithms
3. Confusion matrices
4. ROC and PR curves
5. Calibration summary
6. Selected decision threshold rationale

## 3.11 System Integration Procedure

After model selection, the best-performing model is integrated into the existing web-based Faculty Portfolio Management System as an early warning component.

Integration includes:

1. Risk scoring endpoint/service
2. At-risk tagging logic for faculty-class offerings
3. Chair dashboard indicators (risk level and priority queue)
4. Logging of prediction outputs for monitoring and future retraining

The deployed component is evaluated for consistency between offline test results and in-app inference behavior.

## 3.12 Ethical Considerations

The study adheres to ethical and data privacy principles for institutional data analytics:

1. Use of authorized institutional data only
2. De-identification or pseudonymization of personal data before model development
3. Access control for raw and processed datasets
4. Storage of datasets and model artifacts in secured research directories
5. Reporting in aggregate form without exposing personally identifiable information
6. Compliance with applicable institutional research and data privacy policies

The model is used as **decision support**, not as an autonomous decision-maker. Final intervention actions remain under authorized academic personnel.

## 3.13 Methodological Delimitations

1. The model is trained only on variables available in the current system database.
2. Prediction quality depends on historical data volume and quality.
3. External factors not captured in the system (e.g., personal emergencies) are outside model scope.
4. Results are context-bound to the institution and implementation period covered by the dataset.

## 3.14 Chapter Summary

This chapter established a quantitative predictive methodology for developing and evaluating an ML-based early warning system for faculty portfolio submission risk. The approach combines reproducible data preparation, supervised classification modeling, robust evaluation metrics, statistical model comparison, and deployment-oriented integration into the existing web application.
