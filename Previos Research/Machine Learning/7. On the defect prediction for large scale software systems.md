**Study ID/Title:** Satya Pradhan et al. (2020) - "On the Defect Prediction for Large Scale Software Systems – From Defect Density to Machine Learning"

**Objective/Target variable:**

- **Objective:** To address "Data Definition" and "SDP Lifecycle" challenges in defect prediction for large-scale software and develop models for continuous quality management throughout the development lifecycle (SDLC).
- **Target variable:** Defect Count (Regression) predicted at three stages:
    - **CC Model:** Defects found between Code Complete (CC) and Feature Complete (FC).
    - **FC Model:** Defects found between FC and First Customer Availability (FCA).
    - **FCA Model:** Defects found in the customer environment within 6 months of release.

**Dataset:**

- **Source:** Cisco IOS-XE software (Large-scale commercial software with 200+ million LOC and 2200+ components).
- **Sample size (n):**
    - **Training:** First 6 releases (1,122 observations used in defect density analysis).
    - **Testing:** Last 2 releases (374 observations used in defect density analysis).
- **Features (count + key features):**
    - **CC Model:** 2 to 6 attributes depending on scenario (New feature KLOC, Bug fix KLOC, Number of features).
    - **FC Model:** 27 to 37 attributes selected from 123. Key features: CC metrics + Outstanding defects, Defect arrival rate, Defect MTTR (Mean Time To Resolution).
    - **FCA Model:** 15 to 35 attributes selected from 119. Key features: FC metrics + Release quality criteria, Security metrics, Test effectiveness, Defect backlog.
- **Class balance (counts or %):** N/A (Regression task).

**Preprocessing:**

- **Missing values handling:** Attributes with null values, duplicates, and outliers were removed.
- **Encoding/scaling:** NR
- **Feature selection (if any):** Stepwise Variance Inflation Factor (VIF) analysis was used to remove highly correlated attributes (tested thresholds VIF ≤ 5 and VIF ≤ 10).
- **Imbalance handling (SMOTE/class weights/undersampling/etc.):** N/A

**Models compared:**

- **Baselines:** Defect Density Models (Linear Regression using KLOC).
- **ML Models:**
    - Linear Regression.
    - Random Forest Regression.
    - XGBoost Regression.

**Validation design:**

- **Train/test split OR k-fold (k=?):** Chronological split: Data from the first 6 releases used for training, and data from the last 2 releases used for testing.
- **Any external validation?:** Tested on internal industrial software (Cisco IOS-XE).

**Metrics reported:**

- **Accuracy:** NR
- **Precision:** NR
- **Recall:** NR
- **F1-score:** NR
- **ROC-AUC:** NR
- **PR-AUC:** NR
- **Calibration (Brier/calibration curve):** NR
- **Best model + best values:**
    - **CC Stage:** XGBoost (Scenario 2) - RMSE: 96, MAE: 42.
    - **FC Stage:** Linear Regression (VIF ≤ 10) - RMSE: 32, MAE: 12.
    - **FCA Stage:** Random Forest (VIF ≤ 5) - RMSE: 32, MAE: 12.

**Statistical test/comparison:**

- **P-values:** Reported for defect density models (< 0.0001).
- **Comparison:** Compared prediction errors between "Product-based" and "Component-based" data definitions.

**Explainability:**

- **Feature importance/SHAP/LIME/etc.:** NR (VIF used for selection, but specific importance scores not listed).
- **Top predictors:** NR (General categories like "Process Metrics" and "Size Metrics" discussed).

**Key findings (2-4 bullets):**

- **Data Definition Matters:** Component-based models yielded significantly lower prediction errors compared to product-based models (e.g., FCA prediction error dropped from 11.7% to 1.4%).
- **Lifecycle Approach:** Using multiple models (CC, FC, FCA) with process metrics reduced the overall prediction error to 8.7%, compared to 14.1% for a single defect density model.
- **Model Performance:** Traditional defect density models had high prediction errors (~28%). Machine learning models utilizing process metrics (like test effectiveness and defect arrival rates) significantly outperformed simple size-based models.

**Limitations (from paper):**

- **Measurement Cost:** Code metrics like cyclomatic complexity and modularity could not be included in the Code Complete (CC) model due to the difficulty and high cost of measurement in such a large system.
- **Generalizability:** The study is specific to Cisco IOS-XE; while it suggests general applicability, the specific solutions may not directly apply to other systems without modification.

**Deployment/implementation notes (if any):**

- **Continuous Management:** The paper proposes implementing these models to enable continuous quality management at three distinct milestones (CC, FC, FCA) rather than just a single pre-release assessment.
- **Operational Procedure:** The authors plan to develop operational procedures to use these models for managing quality in upcoming software releases.